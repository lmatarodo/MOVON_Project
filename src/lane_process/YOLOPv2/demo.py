# lane_process/YOLOPv2/demo.py

import argparse
import time
from pathlib import Path
import cv2
import torch
import numpy as np
import serial

try:
    arduino = serial.Serial('/dev/ttyACM0', 9600, timeout=1)
    time.sleep(2)  # ì‹œë¦¬ì–¼ í¬íŠ¸ ì•ˆì •í™” ëŒ€ê¸°
    print("âœ… Arduino ì—°ê²° ì„±ê³µ!")
except serial.SerialException as e:
    print(f"âš ï¸ Arduino ì—°ê²° ì‹¤íŒ¨: {e}")
    arduino = None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ, arduino ë³€ìˆ˜ë¥¼ Noneìœ¼ë¡œ ì„¤ì •

# í•„ìš”í•œ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸ (ìƒëŒ€ ê²½ë¡œ)
from .utils.utils import (
    time_synchronized, select_device, increment_path,
    scale_coords, xyxy2xywh, non_max_suppression, split_for_trace_model,
    driving_area_mask, lane_line_mask, plot_one_box, show_seg_result,
    AverageMeter, LoadImages
)

def make_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, default='data/weights/yolopv2.pt', help='model.pt path(s)')
    parser.add_argument('--source', type=str, default='data/tunnel_drive.mp4', help='source')  # file/folder, 0 for webcam
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.3, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    return parser

def run_demo(opt, shared_data):
    with torch.no_grad():
        detect(opt, shared_data)

def detect(opt, shared_data):
    # -------------------------------------------------
    # 1) ê¸°ë³¸ ì„¤ì • ë° ë””ë ‰í† ë¦¬
    # -------------------------------------------------
    source, weights, save_txt, imgsz = opt.source, opt.weights, opt.save_txt, opt.img_size
    save_img = not opt.nosave and not source.endswith('.txt')  # save inference images

    save_dir = Path(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))
    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)

    inf_time = AverageMeter()
    waste_time = AverageMeter()
    nms_time = AverageMeter()

    lane_departure_start = None
    lane_departure_duration = 0.5  # ì°¨ì„  ì´íƒˆë¡œ íŒë‹¨í•  ì§€ì† ì‹œê°„ (ì´ˆ)

    # -------------------------------------------------
    # 2) ëª¨ë¸ ë¡œë“œ
    # -------------------------------------------------
    stride = 32
    model = torch.jit.load(weights)
    device = select_device(opt.device)
    half = (device.type != 'cpu')
    model = model.to(device).eval()
    if half:
        model.half()  # to FP16

    # -------------------------------------------------
    # 3) ë°ì´í„° ë¡œë” ì¤€ë¹„
    # -------------------------------------------------
    vid_path, vid_writer = None, None
    dataset = LoadImages(source, img_size=imgsz, stride=stride)

    # GPU Warmup (if CUDA)
    if device.type != 'cpu':
        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))

    t0 = time.time()

    # -------------------------------------------------
    # 4) íƒ‘ë·°(Perspective Transform) ì¢Œí‘œ ì •ì˜ 
    # -------------------------------------------------
    #  (ì‚¬ìš© í™˜ê²½ì— ë§ê²Œ ë³€ê²½ í•„ìš”)
    src_points = np.float32([
        [500, 600],   # ì¢Œì¸¡ ìƒë‹¨
        [700, 600],   # ìš°ì¸¡ ìƒë‹¨
        [900, 700],  # ìš°ì¸¡ í•˜ë‹¨
        [300, 700],   # ì¢Œì¸¡ í•˜ë‹¨
    ])
    dst_width, dst_height = 600, 700
    dst_points = np.float32([
        [0, 0],
        [dst_width, 0],
        [dst_width, dst_height],
        [0, dst_height],
    ])
    # ë³€í™˜ í–‰ë ¬ ê³„ì‚°
    M = cv2.getPerspectiveTransform(src_points, dst_points)

    # -------------------------------------------------
    # 5) ë©”ì¸ ì¶”ë¡  ë£¨í”„
    # -------------------------------------------------
    for path, img, im0s, vid_cap in dataset:
        # 5.1) ì „ì²˜ë¦¬
        img = torch.from_numpy(img).to(device)
        img = img.half() if half else img.float()
        img /= 255.0  # [0,255] â†’ [0.0,1.0]

        if img.ndimension() == 3:
            img = img.unsqueeze(0)

        # -------------------------------------------------
        # 5.2) ëª¨ë¸ ì¶”ë¡ 
        # -------------------------------------------------
        t1 = time_synchronized()
        [pred, anchor_grid], seg, ll = model(img)
        t2 = time_synchronized()

        # waste time
        tw1 = time_synchronized()
        pred = split_for_trace_model(pred, anchor_grid)
        tw2 = time_synchronized()

        # NMS
        t3 = time_synchronized()
        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres,
                                   classes=opt.classes, agnostic=opt.agnostic_nms)
        t4 = time_synchronized()

        # -------------------------------------------------
        # 5.3) ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ (ì£¼í–‰ê°€ëŠ¥ì˜ì—­ / ì°¨ì„ )
        # -------------------------------------------------
        da_seg_mask = driving_area_mask(seg)  # (H,W), 0/1
        ll_seg_mask = lane_line_mask(ll)      # (H,W), 0/1

        # -------------------------------------------------
        # 5.4) ê²€ì¶œëœ ê°ì²´ë“¤ í›„ì²˜ë¦¬
        # -------------------------------------------------
        for i, det in enumerate(pred):
            p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)
            s += '%gx%g ' % img.shape[2:]

            if len(det):
                # scale coords
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()
                for *xyxy, conf, cls in reversed(det):
                    if save_img:
                        plot_one_box(xyxy, im0, line_thickness=3)

            # ì¶”ë¡  ì‹œê°„ ì¶œë ¥
            #print(f'{s}Done. ({t2 - t1:.3f}s)')

            #------------------------------------------------
            # 6) ì›ë³¸ì— ì„¸ê·¸ë©˜í…Œì´ì…˜ í‘œì‹œ
            #------------------------------------------------
            show_seg_result(im0, (da_seg_mask, ll_seg_mask), is_demo=True)

            #------------------------------------------------
            # 7) ì°¨ì„  ë§ˆìŠ¤í¬ë¥¼ ë³„ë„ ìœˆë„ìš°ì— (0/255) ì‹œê°í™”
            #------------------------------------------------
            # ll_seg_maskëŠ” 0/1 í…ì„œì´ë¯€ë¡œ 0/255ë¡œ ë³€í™˜
            if isinstance(ll_seg_mask, torch.Tensor):
                ll_mask_np = ll_seg_mask[0].cpu().numpy()  # batch ì°¨ì› ì£¼ì˜. í•„ìš” ì‹œ [0] ì‚¬ìš©
            else:
                ll_mask_np = ll_seg_mask
            # 0/1 â†’ 0/255
            lane_mask_vis = (ll_mask_np * 255).astype(np.uint8)
            #cv2.imshow('lane_mask_only', lane_mask_vis) # ì°¨ì„  ë§ˆìŠ¤í¬ë§Œ
            top_view_img = cv2.warpPerspective(im0, M, (dst_width, dst_height))

 
            lane_3ch = np.dstack([lane_mask_vis, lane_mask_vis, lane_mask_vis])
            lane_top_view = cv2.warpPerspective(lane_3ch, M, (dst_width, dst_height))

            # ì°¨ëŸ‰ ì¤‘ì‹¬ê³¼ ì„ê³„ê°’ ë²”ìœ„
            car_x = dst_width // 2  # ì°¨ëŸ‰ ì¤‘ì‹¬ (íƒ‘ë·° ì´ë¯¸ì§€ì˜ ë„ˆë¹„ ì¤‘ì•™)
            threshold_x = 100  # ê°€ë¡œ ë°©í–¥ ì„ê³„ê°’
            threshold_y_top = 500  # ì„¸ë¡œ ë°©í–¥ ìƒë‹¨ ì œí•œ
            threshold_y_bottom = 700  # ì„¸ë¡œ ë°©í–¥ í•˜ë‹¨ ì œí•œ

            # ì¤‘ì‹¬ì„ ê³¼ ì„ê³„ê°’ ë²”ìœ„ í‘œì‹œ
            cv2.line(lane_top_view, (car_x, 0), (car_x, dst_height), (255, 0, 0), 2)  # íŒŒë€ìƒ‰ ì¤‘ì‹¬ì„ 
            cv2.rectangle(lane_top_view,
                        (car_x - threshold_x, threshold_y_top),  # ì™¼ìª½ ìƒë‹¨
                        (car_x + threshold_x, threshold_y_bottom),  # ì˜¤ë¥¸ìª½ í•˜ë‹¨
                        (0, 255, 255), 2)  # ë…¸ë€ìƒ‰ í…Œë‘ë¦¬ (ì„ê³„ê°’ ë²”ìœ„)

            # ê´€ì‹¬ ì˜ì—­ ë‚´ ì°¨ì„  ë§ˆìŠ¤í¬ í™•ì¸
            mask_roi = lane_top_view[threshold_y_top:threshold_y_bottom, car_x - threshold_x:car_x + threshold_x]  # ì œí•œëœ ROI
            white_pixels = np.count_nonzero(mask_roi)  # í°ìƒ‰ í”½ì…€(ì°¨ì„ ) ê°œìˆ˜

            # ë””ë²„ê¹…ìš© ì¶œë ¥
            #print("ROI Pixel Values:", np.unique(mask_roi))  # ê³ ìœ  í”½ì…€ ê°’ í™•ì¸
            #print("Number of White Pixels in ROI:", white_pixels)

            prev_lane_status = shared_data['lane_departure']
            lane_departure_detected = False
            # ì´íƒˆ ì—¬ë¶€ íŒë‹¨
            if white_pixels > 2973:  # ì„ê³„ê°’ ë²”ìœ„ ë‚´ì— í°ìƒ‰ í”½ì…€ì´ ìˆë‹¤ë©´
                if lane_departure_start is None:
                    lane_departure_start = time.time()
                
                elapsed_time = time.time() - lane_departure_start
                if elapsed_time >= lane_departure_duration:
                    lane_departure_detected = True
                    if prev_lane_status != lane_departure_detected:
                        arduino.write(b'L')  # ì•„ë‘ì´ë…¸ì— 'L' ì „ì†¡ â†’ LED ì¼œì§
                        print(f"ğŸš¨ ì°¨ì„  ë²—ì–´ë‚¨")
                        print("ğŸ”” ì•„ë‘ì´ë…¸ ì‹ í˜¸ ì „ì†¡ (LED ì¼œì§)")
                    #print("âš ï¸ ì°¨ëŸ‰ì´ ì°¨ì„  ì¤‘ì•™ì—ì„œ ì´íƒˆí–ˆìŠµë‹ˆë‹¤!")
                    cv2.putText(lane_top_view, "WARNING: Lane Departure", (50, 50),
                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            else:
                lane_departure_start = None
                if prev_lane_status != lane_departure_detected:
                    print(f"ğŸš¨ ì°¨ì„  ëŒì•„ì˜´")
                    print("ğŸ”” ì•„ë‘ì´ë…¸ ì‹ í˜¸ ì „ì†¡ (LED êº¼ì§)")
                    arduino.write(b'l') # ì•„ë‘ì´ë…¸ì— 'l' ì „ì†¡ â†’ LED êº¼ì§
                #print("âœ… ì°¨ëŸ‰ì´ ì°¨ì„  ì¤‘ì•™ì— ìˆìŠµë‹ˆë‹¤.")
                cv2.putText(lane_top_view, "Lane Centered", (50, 50),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            #if prev_lane_status != lane_departure_detected:
                    #print("âœ… [YOLO] ì°¨ì„  ì´íƒˆ ìƒíƒœ ë³€ê²½: {lane_departure_detected}")
            shared_data['lane_departure'] = lane_departure_detected



            # ë””ë²„ê¹…: ROIì™€ íƒ‘ë·° ì¶œë ¥
            # cv2.imshow('ROI', mask_roi)
            cv2.imshow('lane_top_view_with_visuals', lane_top_view)  # ì°¨ì„  ë§ˆìŠ¤í¬ íƒ‘ë·°

            cv2.imshow('lane_view', im0)           # ì›ë³¸
            #cv2.imshow('top_view', top_view_img)  # ì›ë³¸ íƒ‘ë·°

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break  # q í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¤‘ë‹¨

            #------------------------------------------------
            # 10) ê²°ê³¼ ì €ì¥
            #------------------------------------------------
            # if save_img:
            #     p = Path(p)  # Path ê°ì²´
            #     save_path = str(save_dir / p.name)
            #     if dataset.mode == 'image':
            #         cv2.imwrite(save_path, im0)
            #         print(f" The image with the result is saved in: {save_path}")
            #     else:  # 'video' or 'stream'
            #         if vid_path != save_path:
            #             vid_path = save_path
            #             if isinstance(vid_writer, cv2.VideoWriter):
            #                 vid_writer.release()
            #             if vid_cap:  # video
            #                 fps = vid_cap.get(cv2.CAP_PROP_FPS)
            #                 w, h = im0.shape[1], im0.shape[0]
            #             else:  # stream
            #                 fps, w, h = 30, im0.shape[1], im0.shape[0]
            #                 save_path += '.mp4'
            #             vid_writer = cv2.VideoWriter(save_path,
            #                                          cv2.VideoWriter_fourcc(*'mp4v'),
            #                                          fps, (w, h))
            #         vid_writer.write(im0)

        # ë°”ê¹¥ forë¬¸ìš©, 'q'ë¡œ ì™„ì „ ì¢…ë£Œ
        if key == ord('q'):
            break

    if arduino:
        arduino.close()    

    # -------------------------------------------------
    # 11) ì‹œê°„ ì¶œë ¥
    # -------------------------------------------------
    inf_time.update(t2 - t1, img.size(0))
    nms_time.update(t4 - t3, img.size(0))
    waste_time.update(tw2 - tw1, img.size(0))
    print('inf : (%.4fs/frame)   nms : (%.4fs/frame)' % (inf_time.avg, nms_time.avg))
    print(f'Done. ({time.time() - t0:.3f}s)')

    # ëª¨ë“  ì°½ ë‹«ê¸°
    cv2.destroyAllWindows()

if __name__ == '__main__':
    opt = make_parser().parse_args()
    #print(opt)
    run_demo(opt)
    
